train_file: ['/mnt/data/yangzhenbang/datasets/blip_caption/data_coco_vg_flickr/kg_data/coco_train_has_knowledge.json',
             '/mnt/data/yangzhenbang/datasets/blip_caption/data_coco_vg_flickr/kg_data/vg_has_knowledge.json',
             ]
image_knowledge: ['/mnt/data/yangzhenbang/datasets/blip_caption/data_coco_vg_flickr/kg_data/coco_train_visual_knowledge.json',
             '/mnt/data/yangzhenbang/datasets/blip_caption/data_coco_vg_flickr/kg_data/vg_visual_knowledge.json',
             ]
##forward测试
#train_file: [
#             '/mnt/data/yangzhenbang/datasets/blip_caption/data_coco_vg_flickr/kg_data/vg_has_knowledge.json',
#             ]
#image_knowledge: ['/mnt/data/yangzhenbang/datasets/blip_caption/data_coco_vg_flickr/kg_data/vg_example_visual_knowledge.json',
#             ]
pretrained: 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth'

vit: 'base'
#batch_size_train: 16
#batch_size_test: 16
batch_size: 15
vit_grad_ckpt: True
vit_ckpt_layer: 4
#init_lr: 1e-5

laion_path: ''   
image_root: '/mnt/data/datasets/coco/coco2014/'
# size of vit model; base or large
#vit: 'base'
#vit_grad_ckpt: False
#vit_ckpt_layer: 0

image_size: 224
queue_size: 57600
alpha: 0.4

max_words: 200


# optimizer
weight_decay: 0.05
init_lr: 3e-4
min_lr: 1e-6
warmup_lr: 1e-6
lr_decay_rate: 0.9
max_epoch: 10
warmup_steps: 3000

add_meta_net: false

# knowledge
enhance_image: False  #当为True时，使用knowledge增强image；当为false时使用knowledge按一定比例替代caption
vg_or_cc: False #True = vg   ;False = cc
knowledge_ratio: -1  #将多少caption换为knowlwedge
knowledge_num: 1
fix_blip: false






